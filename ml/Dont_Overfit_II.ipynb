{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dont Overfit II",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2PnWFfyknDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46ewpHk3rHWq",
        "colab_type": "text"
      },
      "source": [
        "Input data:\n",
        "- blackbox features\n",
        "- 300 dimensions\n",
        "- input data size: 200\n",
        "- to predict: 19.750"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLOpuYForPJS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5pwOK8dhEMx",
        "colab_type": "code",
        "outputId": "272021b0-6572-4b61-86f4-69e7575c4794",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "df = pd.read_csv('train.csv')\n",
        "print(\"Shape: \", df.shape)\n",
        "train_labels = df['target'].values\n",
        "train_data = df.drop(['id', 'target'], axis=1).values\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:  (250, 302)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>...</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.098</td>\n",
              "      <td>2.165</td>\n",
              "      <td>0.681</td>\n",
              "      <td>-0.614</td>\n",
              "      <td>1.309</td>\n",
              "      <td>-0.455</td>\n",
              "      <td>-0.236</td>\n",
              "      <td>0.276</td>\n",
              "      <td>-2.246</td>\n",
              "      <td>1.825</td>\n",
              "      <td>-0.912</td>\n",
              "      <td>-0.107</td>\n",
              "      <td>0.305</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.826</td>\n",
              "      <td>0.417</td>\n",
              "      <td>0.177</td>\n",
              "      <td>-0.673</td>\n",
              "      <td>-0.503</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.410</td>\n",
              "      <td>-1.927</td>\n",
              "      <td>0.102</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>1.763</td>\n",
              "      <td>1.449</td>\n",
              "      <td>-1.097</td>\n",
              "      <td>-0.686</td>\n",
              "      <td>-0.250</td>\n",
              "      <td>-1.859</td>\n",
              "      <td>1.125</td>\n",
              "      <td>1.009</td>\n",
              "      <td>-2.296</td>\n",
              "      <td>0.385</td>\n",
              "      <td>-0.876</td>\n",
              "      <td>1.528</td>\n",
              "      <td>-0.144</td>\n",
              "      <td>-1.078</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.681</td>\n",
              "      <td>1.250</td>\n",
              "      <td>-0.565</td>\n",
              "      <td>-1.318</td>\n",
              "      <td>-0.923</td>\n",
              "      <td>0.075</td>\n",
              "      <td>-0.704</td>\n",
              "      <td>2.457</td>\n",
              "      <td>0.771</td>\n",
              "      <td>-0.460</td>\n",
              "      <td>0.569</td>\n",
              "      <td>-1.320</td>\n",
              "      <td>-1.516</td>\n",
              "      <td>-2.145</td>\n",
              "      <td>-1.120</td>\n",
              "      <td>0.156</td>\n",
              "      <td>0.820</td>\n",
              "      <td>-1.049</td>\n",
              "      <td>-1.125</td>\n",
              "      <td>0.484</td>\n",
              "      <td>0.617</td>\n",
              "      <td>1.253</td>\n",
              "      <td>1.248</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.802</td>\n",
              "      <td>-0.896</td>\n",
              "      <td>-1.793</td>\n",
              "      <td>-0.284</td>\n",
              "      <td>-0.601</td>\n",
              "      <td>0.569</td>\n",
              "      <td>0.867</td>\n",
              "      <td>1.347</td>\n",
              "      <td>0.504</td>\n",
              "      <td>-0.649</td>\n",
              "      <td>0.672</td>\n",
              "      <td>-2.097</td>\n",
              "      <td>1.051</td>\n",
              "      <td>-0.414</td>\n",
              "      <td>1.038</td>\n",
              "      <td>-1.065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.081</td>\n",
              "      <td>-0.973</td>\n",
              "      <td>-0.383</td>\n",
              "      <td>0.326</td>\n",
              "      <td>-0.428</td>\n",
              "      <td>0.317</td>\n",
              "      <td>1.172</td>\n",
              "      <td>0.352</td>\n",
              "      <td>0.004</td>\n",
              "      <td>-0.291</td>\n",
              "      <td>2.907</td>\n",
              "      <td>1.085</td>\n",
              "      <td>2.144</td>\n",
              "      <td>1.540</td>\n",
              "      <td>0.584</td>\n",
              "      <td>1.133</td>\n",
              "      <td>1.098</td>\n",
              "      <td>-0.237</td>\n",
              "      <td>-0.498</td>\n",
              "      <td>0.283</td>\n",
              "      <td>-1.100</td>\n",
              "      <td>-0.417</td>\n",
              "      <td>1.382</td>\n",
              "      <td>-0.515</td>\n",
              "      <td>-1.519</td>\n",
              "      <td>0.619</td>\n",
              "      <td>-0.128</td>\n",
              "      <td>0.866</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>1.238</td>\n",
              "      <td>-0.227</td>\n",
              "      <td>0.269</td>\n",
              "      <td>-0.390</td>\n",
              "      <td>-2.721</td>\n",
              "      <td>1.659</td>\n",
              "      <td>0.106</td>\n",
              "      <td>-0.121</td>\n",
              "      <td>1.719</td>\n",
              "      <td>...</td>\n",
              "      <td>0.971</td>\n",
              "      <td>-1.489</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.917</td>\n",
              "      <td>-0.094</td>\n",
              "      <td>-1.407</td>\n",
              "      <td>0.887</td>\n",
              "      <td>-0.104</td>\n",
              "      <td>-0.583</td>\n",
              "      <td>1.267</td>\n",
              "      <td>-1.667</td>\n",
              "      <td>-2.771</td>\n",
              "      <td>-0.516</td>\n",
              "      <td>1.312</td>\n",
              "      <td>0.491</td>\n",
              "      <td>0.932</td>\n",
              "      <td>2.064</td>\n",
              "      <td>0.422</td>\n",
              "      <td>1.215</td>\n",
              "      <td>2.012</td>\n",
              "      <td>0.043</td>\n",
              "      <td>-0.307</td>\n",
              "      <td>-0.059</td>\n",
              "      <td>1.121</td>\n",
              "      <td>1.333</td>\n",
              "      <td>0.211</td>\n",
              "      <td>1.753</td>\n",
              "      <td>0.053</td>\n",
              "      <td>1.274</td>\n",
              "      <td>-0.612</td>\n",
              "      <td>-0.165</td>\n",
              "      <td>-1.695</td>\n",
              "      <td>-1.257</td>\n",
              "      <td>1.359</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>-1.624</td>\n",
              "      <td>-0.458</td>\n",
              "      <td>-1.099</td>\n",
              "      <td>-0.936</td>\n",
              "      <td>0.973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.089</td>\n",
              "      <td>-0.348</td>\n",
              "      <td>0.148</td>\n",
              "      <td>-0.022</td>\n",
              "      <td>0.404</td>\n",
              "      <td>-0.023</td>\n",
              "      <td>-0.172</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.183</td>\n",
              "      <td>0.459</td>\n",
              "      <td>0.478</td>\n",
              "      <td>-0.425</td>\n",
              "      <td>0.352</td>\n",
              "      <td>1.095</td>\n",
              "      <td>0.300</td>\n",
              "      <td>-1.044</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.038</td>\n",
              "      <td>0.144</td>\n",
              "      <td>-1.658</td>\n",
              "      <td>-0.946</td>\n",
              "      <td>0.633</td>\n",
              "      <td>-0.772</td>\n",
              "      <td>1.786</td>\n",
              "      <td>0.136</td>\n",
              "      <td>-0.103</td>\n",
              "      <td>-1.223</td>\n",
              "      <td>2.273</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-2.032</td>\n",
              "      <td>-0.452</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.924</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>-0.067</td>\n",
              "      <td>-0.917</td>\n",
              "      <td>1.896</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.540</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>1.074</td>\n",
              "      <td>-0.748</td>\n",
              "      <td>1.086</td>\n",
              "      <td>-0.766</td>\n",
              "      <td>-0.931</td>\n",
              "      <td>0.432</td>\n",
              "      <td>1.345</td>\n",
              "      <td>-0.491</td>\n",
              "      <td>-1.602</td>\n",
              "      <td>-0.727</td>\n",
              "      <td>0.346</td>\n",
              "      <td>0.780</td>\n",
              "      <td>-0.527</td>\n",
              "      <td>-1.122</td>\n",
              "      <td>-0.208</td>\n",
              "      <td>-0.730</td>\n",
              "      <td>-0.302</td>\n",
              "      <td>2.535</td>\n",
              "      <td>-1.045</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.020</td>\n",
              "      <td>1.373</td>\n",
              "      <td>0.456</td>\n",
              "      <td>-0.277</td>\n",
              "      <td>1.381</td>\n",
              "      <td>1.843</td>\n",
              "      <td>0.749</td>\n",
              "      <td>0.202</td>\n",
              "      <td>0.013</td>\n",
              "      <td>0.263</td>\n",
              "      <td>-1.222</td>\n",
              "      <td>0.726</td>\n",
              "      <td>1.444</td>\n",
              "      <td>-1.165</td>\n",
              "      <td>-1.544</td>\n",
              "      <td>0.004</td>\n",
              "      <td>0.800</td>\n",
              "      <td>-1.211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>0.392</td>\n",
              "      <td>-1.637</td>\n",
              "      <td>-0.446</td>\n",
              "      <td>-0.725</td>\n",
              "      <td>-1.035</td>\n",
              "      <td>0.834</td>\n",
              "      <td>0.503</td>\n",
              "      <td>0.274</td>\n",
              "      <td>0.335</td>\n",
              "      <td>-1.148</td>\n",
              "      <td>0.067</td>\n",
              "      <td>-1.010</td>\n",
              "      <td>1.048</td>\n",
              "      <td>-1.442</td>\n",
              "      <td>0.210</td>\n",
              "      <td>0.836</td>\n",
              "      <td>-0.326</td>\n",
              "      <td>0.716</td>\n",
              "      <td>-0.764</td>\n",
              "      <td>0.248</td>\n",
              "      <td>-1.308</td>\n",
              "      <td>2.127</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.296</td>\n",
              "      <td>-0.808</td>\n",
              "      <td>1.854</td>\n",
              "      <td>0.118</td>\n",
              "      <td>0.380</td>\n",
              "      <td>0.999</td>\n",
              "      <td>-1.171</td>\n",
              "      <td>2.798</td>\n",
              "      <td>0.394</td>\n",
              "      <td>-1.048</td>\n",
              "      <td>1.078</td>\n",
              "      <td>0.401</td>\n",
              "      <td>-0.486</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.083</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>1.251</td>\n",
              "      <td>-0.206</td>\n",
              "      <td>-0.933</td>\n",
              "      <td>-1.215</td>\n",
              "      <td>0.281</td>\n",
              "      <td>0.512</td>\n",
              "      <td>-0.424</td>\n",
              "      <td>0.769</td>\n",
              "      <td>0.223</td>\n",
              "      <td>-0.710</td>\n",
              "      <td>2.725</td>\n",
              "      <td>0.176</td>\n",
              "      <td>0.845</td>\n",
              "      <td>-1.226</td>\n",
              "      <td>1.527</td>\n",
              "      <td>-1.701</td>\n",
              "      <td>0.597</td>\n",
              "      <td>0.150</td>\n",
              "      <td>1.864</td>\n",
              "      <td>0.322</td>\n",
              "      <td>-0.214</td>\n",
              "      <td>1.282</td>\n",
              "      <td>0.408</td>\n",
              "      <td>-0.910</td>\n",
              "      <td>1.020</td>\n",
              "      <td>-0.299</td>\n",
              "      <td>-1.574</td>\n",
              "      <td>-1.618</td>\n",
              "      <td>-0.404</td>\n",
              "      <td>0.640</td>\n",
              "      <td>-0.595</td>\n",
              "      <td>-0.966</td>\n",
              "      <td>0.900</td>\n",
              "      <td>0.467</td>\n",
              "      <td>-0.562</td>\n",
              "      <td>-0.254</td>\n",
              "      <td>-0.533</td>\n",
              "      <td>0.238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.347</td>\n",
              "      <td>-0.831</td>\n",
              "      <td>0.511</td>\n",
              "      <td>-0.021</td>\n",
              "      <td>1.225</td>\n",
              "      <td>1.594</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1.509</td>\n",
              "      <td>-0.012</td>\n",
              "      <td>2.198</td>\n",
              "      <td>0.190</td>\n",
              "      <td>0.453</td>\n",
              "      <td>0.494</td>\n",
              "      <td>1.478</td>\n",
              "      <td>-1.412</td>\n",
              "      <td>0.270</td>\n",
              "      <td>-1.312</td>\n",
              "      <td>-0.322</td>\n",
              "      <td>-0.688</td>\n",
              "      <td>-0.198</td>\n",
              "      <td>-0.285</td>\n",
              "      <td>1.042</td>\n",
              "      <td>-0.315</td>\n",
              "      <td>-0.478</td>\n",
              "      <td>0.024</td>\n",
              "      <td>-0.190</td>\n",
              "      <td>1.656</td>\n",
              "      <td>-0.469</td>\n",
              "      <td>-1.437</td>\n",
              "      <td>-0.581</td>\n",
              "      <td>-0.308</td>\n",
              "      <td>-0.837</td>\n",
              "      <td>-1.739</td>\n",
              "      <td>0.037</td>\n",
              "      <td>0.336</td>\n",
              "      <td>-1.102</td>\n",
              "      <td>2.371</td>\n",
              "      <td>0.554</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.050</td>\n",
              "      <td>-0.347</td>\n",
              "      <td>0.904</td>\n",
              "      <td>-1.324</td>\n",
              "      <td>-0.849</td>\n",
              "      <td>3.432</td>\n",
              "      <td>0.222</td>\n",
              "      <td>0.416</td>\n",
              "      <td>0.174</td>\n",
              "      <td>-1.517</td>\n",
              "      <td>-0.337</td>\n",
              "      <td>0.055</td>\n",
              "      <td>-0.464</td>\n",
              "      <td>0.014</td>\n",
              "      <td>-1.073</td>\n",
              "      <td>0.325</td>\n",
              "      <td>-0.523</td>\n",
              "      <td>-0.692</td>\n",
              "      <td>0.190</td>\n",
              "      <td>-0.883</td>\n",
              "      <td>-1.830</td>\n",
              "      <td>1.408</td>\n",
              "      <td>2.319</td>\n",
              "      <td>1.704</td>\n",
              "      <td>-0.723</td>\n",
              "      <td>1.014</td>\n",
              "      <td>0.064</td>\n",
              "      <td>0.096</td>\n",
              "      <td>-0.775</td>\n",
              "      <td>1.845</td>\n",
              "      <td>0.898</td>\n",
              "      <td>0.134</td>\n",
              "      <td>2.415</td>\n",
              "      <td>-0.996</td>\n",
              "      <td>-1.006</td>\n",
              "      <td>1.378</td>\n",
              "      <td>1.246</td>\n",
              "      <td>1.478</td>\n",
              "      <td>0.428</td>\n",
              "      <td>0.253</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 302 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target      0      1      2  ...    295    296    297    298    299\n",
              "0   0     1.0 -0.098  2.165  0.681  ... -2.097  1.051 -0.414  1.038 -1.065\n",
              "1   1     0.0  1.081 -0.973 -0.383  ... -1.624 -0.458 -1.099 -0.936  0.973\n",
              "2   2     1.0 -0.523 -0.089 -0.348  ... -1.165 -1.544  0.004  0.800 -1.211\n",
              "3   3     1.0  0.067 -0.021  0.392  ...  0.467 -0.562 -0.254 -0.533  0.238\n",
              "4   4     1.0  2.347 -0.831  0.511  ...  1.378  1.246  1.478  0.428  0.253\n",
              "\n",
              "[5 rows x 302 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKXxzD6-rxgk",
        "colab_type": "text"
      },
      "source": [
        "StandardScaler will transform the data such that its distribution will have a mean value 0 and standard deviation of 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNVQfAu6e0di",
        "colab_type": "code",
        "outputId": "b7dc162b-7160-4c30-9aa4-bc53ef210877",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "train_data = StandardScaler().fit_transform(train_data)\n",
        "train_data"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.12173567,  2.17600225,  0.5036917 , ..., -0.42172402,\n",
              "         1.02241106, -0.96572011],\n",
              "       [ 1.06157697, -0.93927777, -0.53978988, ..., -1.1087964 ,\n",
              "        -0.93965825,  1.13688315],\n",
              "       [-0.54829027, -0.06167818, -0.50546483, ..., -0.00245942,\n",
              "         0.78584951, -1.11634821],\n",
              "       ...,\n",
              "       [ 1.39278408, -1.42771556, -0.04747057, ...,  0.75582776,\n",
              "         0.15766927, -0.60875213],\n",
              "       [ 0.59889069,  1.05914755,  0.01627596, ..., -0.81390694,\n",
              "         2.00742153, -0.30336913],\n",
              "       [ 0.46741151,  0.42675961, -0.02785625, ..., -1.46487041,\n",
              "        -0.63053791,  1.65376453]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BplasQoisKWD",
        "colab_type": "text"
      },
      "source": [
        "<strong>Recursive feature elimination</strong> is a backward selection of the predictors. This technique begins by building a model on the entire set of predictors and computing an importance score for each predictor. The least important predictor are then removed, the model is re-built, and importance scores are computed again. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDtCXg-bukyE",
        "colab_type": "code",
        "outputId": "1f220fa6-da0a-4309-b24c-8359892cb4f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "n_features = 25\n",
        "selector_name = \"rfe\"\n",
        "selectors = {\n",
        "    \"k_best\": SelectKBest(f_classif, k=n_features),\n",
        "    \"rfe\": RFE(LinearSVC(max_iter=5000), n_features)\n",
        "}\n",
        "selector = selectors[selector_name]\n",
        "\n",
        "selected_features = selector.fit_transform(train_data, train_labels)\n",
        "\n",
        "if hasattr(selector, 'scores_'):\n",
        "  feature_indexes = (-selector.scores_).argsort()[:n_features]\n",
        "else:\n",
        "  feature_indexes = selector.ranking_.argsort()[:n_features]\n",
        "\n",
        "print(feature_indexes)\n",
        "selected_train_data = train_data[:, feature_indexes]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[227 264 272  91  33 261  95 258  98 281 199  73 194 101 285 108  43 298\n",
            "  46 209  65 295 217  13 228]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhN_ZUbzqmK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import model_selection\n",
        "kfold = model_selection.KFold(n_splits=5)\n",
        "n_estimators = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwGdYK9XX-SL",
        "colab_type": "code",
        "outputId": "98f5c2ac-9fab-4154-fa41-548321b60f50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "log_clf = LogisticRegression()\n",
        "param_grid = {\n",
        "  'class_weight' : ['balanced', None], \n",
        "  'penalty' : ['l2','l1'], \n",
        "  'solver': ['liblinear'] ,\n",
        "  'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(estimator = log_clf, cv=3, scoring=\"roc_auc\", param_grid = param_grid, n_jobs = -1)\n",
        "\n",
        "grid.fit(selected_train_data, train_labels)\n",
        "\n",
        "print(\"Best Score:\" + str(grid.best_score_))\n",
        "print(\"Best Parameters: \" + str(grid.best_params_))\n",
        "\n",
        "best_parameters = grid.best_params_\n",
        "\n",
        "model = grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best Score:0.876338784067086\n",
            "Best Parameters: {'C': 1, 'class_weight': None, 'penalty': 'l2', 'solver': 'liblinear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU5zaTZoxQSv",
        "colab_type": "text"
      },
      "source": [
        "Bagging: Due to the theoretical variance of the training dataset (we remind that a dataset is an observed sample coming from a true unknown underlying distribution), the fitted model is also subject to variability: if another dataset had been observed, we would have obtained a different model.\n",
        "<strong>The idea of bagging is then simple: we want to fit several independent models and “average” their predictions in order to obtain a model with a lower variance.</strong>\n",
        "\n",
        "Boosting: Intuitively, <strong>each new model focus its efforts on the most difficult observations</strong> to fit up to now, so that we obtain, at the end of the process, a strong learner with lower bias (even if we can notice that boosting can also have the effect of reducing variance). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGJGaU7fqyeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "base_estimator = DecisionTreeClassifier()\n",
        "\n",
        "model = BaggingClassifier(base_estimator=base_estimator, n_estimators=n_estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYr6zjWTqdax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(n_estimators=n_estimators)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjyXQmc5lOwT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = model_selection.cross_val_score(model, selected_train_data, train_labels, cv=kfold)\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0bZE8xUowT_",
        "colab_type": "code",
        "outputId": "abf2c487-5182-4b75-8554-62d0801d0e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "model.fit(selected_train_data, train_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=10,\n",
              "                           n_iter_no_change=None, presort='auto',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lxs8RRTNoEoh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('test.csv')\n",
        "ids = df['id'].values\n",
        "test_data = df.drop('id', axis = 1).values\n",
        "selected_test_data = StandardScaler().fit_transform(test_data[:, feature_indexes])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgYHxhO8omR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = model.predict_proba(selected_test_data)\n",
        "predicted = [p[1] for p in predicted]\n",
        "submission_df = pd.DataFrame({'id': ids, 'target': predicted})\n",
        "submission_df.head()\n",
        "submission_df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4_s3piKdp3",
        "colab_type": "text"
      },
      "source": [
        "<h2>Average Submissions</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFqlGYFIKpnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "submissions_dir = \"./submissions\"\n",
        "files = os.listdir(submissions_dir)\n",
        "\n",
        "sub_df = [pd.read_csv(submissions_dir + \"/\" + file) for file in files if '.csv' in file]\n",
        "mean_df = pd.concat(sub_df).groupby(level=0).mean()\n",
        "mean_df\n",
        "mean_df.to_csv('submission.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}